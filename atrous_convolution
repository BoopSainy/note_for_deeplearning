/*
  Here is the note from https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d
  
  I copy it word by word for learning.
*/

An introduction to different types of convolutions in deep learning

let me give you a quick overview of different types of convolutions and what their benefits are. For the sake of
simplicity, I am focussing on 2D convolutions only.

Convolutions:
  First we need to agree on a few parameters that define a convolutional layer.
  kernel size: the kernel size defines the field of view of the convolution. A common choice for 2d is 3 - that is
3x3 pixels.
  stride: the stride defines the step size of the kernel when traversing the image. While its default is usually 1, 
we can use a stride of 2 for downsampling an image similar to MaxPooling.
  padding: the padding defines how the border of a sample is handled. A half padded convolution will keep the spatial
output dimensions equal to the input, whereas unpadded convolutions will crop away some of the borders if the kernel
is larger than 1.

Dilated convolutions (a.k.a atrous convolutions):
  dilated convolutions introduce another parameter to convolution layers called the dilated rate. This defines a 
spacing between the values in a kernel. A 3x3 kernel with a dilatin rate of 2 will have the same field of view as
a 5x5 kernel, while only using 9 parameters. Imaging taking a 5x5 kernel and deleting every second column and row.
  This delivers a wider field of view at the same computational cost. Dilated convolutions are particularlu popular
in the field of real-time segmentation. Use them if you need a wide field of view and cannot afford multiple 
convolutions or larger kernels.

