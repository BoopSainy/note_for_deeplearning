note from d2l.ai tutorial；
and from paper "aggregated residual transformations for deep neural networks"


recall fig. that each resnet block simply stacks layers between residual connections. this design can be varied by replacing
stacked layers with concatenated parallel transformations, leading to resnext. different from a variety of transformations in
multi-branch inception blocks, resnext adopts the same transformation in all branches, thus minimizing manual design efforts in 
each branch.

the resnext block. it is a bottleneck (when b < c) residual block with group convolution (g group).

the left dotted box in fig depicts the added concatenated parallel transformation strategy in resnext. more concretely, an input
with c channels is first split into g groups via g branches of 1x1 convs followed by 3x3 convs, all with b/g output channels.
concatenating these g output results in b output channels, leading to "bottlenecked" (when b < c) network width inside the dashed 
box. this output will restore the original c channels of the input via the final 1x1 conv right before sum with the residual 
connection. Notably, the left dotted box is equivalent to much simplified right dotted box in fig, where we only need to specify
that the 3x3 conv is a group conv with g groups. in fact, the group conv dates back to the idea of distributing the alexnet
model over two gpus due to limited gpu memory at that time.

以后有时间再细看resnext
